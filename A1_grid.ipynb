{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32da8b20-d097-45af-8af0-27ac68bb0b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from world.delivery_environment import Environment\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b1b2b2-5a17-4270-abc2-283b8eccc4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(Path(\"grid_configs/custom_medium_grid_3.npy\"), no_gui=True, sigma=0.0)\n",
    "env.reset()\n",
    "n_rows, n_cols = env.grid.shape\n",
    "max_deliveries = env.initial_target_count\n",
    "state_dim = 3\n",
    "\n",
    "def encode_state_norm(raw: tuple[int,int,int]) -> torch.Tensor:\n",
    "    i, j, rem = raw\n",
    "    return torch.tensor([\n",
    "        i / (n_rows - 1),\n",
    "        j / (n_cols - 1),\n",
    "        rem\n",
    "    ], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c832a3fb-1c71-43f0-80d6-7edf89a5400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f81470a-831b-4dc4-b252-7a1fe92d806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store raw state tuples for later encoding\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # encode and stack\n",
    "        states_enc = torch.stack([encode_state_norm(s) for s in states])\n",
    "        next_states_enc = torch.stack([encode_state_norm(s) for s in next_states])\n",
    "        return (\n",
    "            states_enc,\n",
    "            torch.tensor(actions, dtype=torch.int64, device=device),\n",
    "            torch.tensor(rewards, dtype=torch.float32, device=device),\n",
    "            next_states_enc,\n",
    "            torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "207f02f1-0591-4451-a50b-c53229a55707",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 4\n",
    "buffer_capacity = 10000\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "target_update_freq = 50\n",
    "num_episodes = 2000\n",
    "max_steps_per_episode = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f559e403-0ef2-49df-8801-c483c9e01469",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "672b4947-1783-4d77-a5e4-62e37921f3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Total Reward: -788.00, Epsilon: 0.95, Steps: 453, Rem Targets: 0.0\n",
      "Episode 20, Total Reward: -1247.00, Epsilon: 0.90, Steps: 600, Rem Targets: 0.0\n",
      "Episode 30, Total Reward: -749.00, Epsilon: 0.86, Steps: 490, Rem Targets: 0.0\n",
      "Episode 40, Total Reward: -1077.00, Epsilon: 0.82, Steps: 600, Rem Targets: 2.0\n",
      "Episode 50, Total Reward: -1117.00, Epsilon: 0.78, Steps: 600, Rem Targets: 2.0\n",
      "Episode 60, Total Reward: -966.00, Epsilon: 0.74, Steps: 600, Rem Targets: 1.0\n",
      "Episode 70, Total Reward: -561.00, Epsilon: 0.70, Steps: 478, Rem Targets: 0.0\n",
      "Episode 80, Total Reward: -990.00, Epsilon: 0.67, Steps: 600, Rem Targets: 1.0\n",
      "Episode 90, Total Reward: -1013.00, Epsilon: 0.64, Steps: 600, Rem Targets: 2.0\n",
      "Episode 100, Total Reward: -938.00, Epsilon: 0.61, Steps: 600, Rem Targets: 1.0\n",
      "Episode 110, Total Reward: -1102.00, Epsilon: 0.58, Steps: 600, Rem Targets: 1.0\n",
      "Episode 120, Total Reward: -906.00, Epsilon: 0.55, Steps: 600, Rem Targets: 1.0\n",
      "Episode 130, Total Reward: -882.00, Epsilon: 0.52, Steps: 571, Rem Targets: 0.0\n",
      "Episode 140, Total Reward: -898.00, Epsilon: 0.50, Steps: 600, Rem Targets: 1.0\n",
      "Episode 150, Total Reward: -885.00, Epsilon: 0.47, Steps: 600, Rem Targets: 2.0\n",
      "Episode 160, Total Reward: -746.00, Epsilon: 0.45, Steps: 600, Rem Targets: 1.0\n",
      "Episode 170, Total Reward: -877.00, Epsilon: 0.43, Steps: 600, Rem Targets: 2.0\n",
      "Episode 180, Total Reward: -861.00, Epsilon: 0.41, Steps: 600, Rem Targets: 2.0\n",
      "Episode 190, Total Reward: -857.00, Epsilon: 0.39, Steps: 600, Rem Targets: 2.0\n",
      "Episode 200, Total Reward: -781.00, Epsilon: 0.37, Steps: 600, Rem Targets: 2.0\n",
      "Episode 210, Total Reward: -813.00, Epsilon: 0.35, Steps: 600, Rem Targets: 2.0\n",
      "Episode 220, Total Reward: -829.00, Epsilon: 0.33, Steps: 600, Rem Targets: 2.0\n",
      "Episode 230, Total Reward: -734.00, Epsilon: 0.32, Steps: 600, Rem Targets: 1.0\n",
      "Episode 240, Total Reward: -738.00, Epsilon: 0.30, Steps: 600, Rem Targets: 1.0\n",
      "Episode 250, Total Reward: -762.00, Epsilon: 0.29, Steps: 600, Rem Targets: 1.0\n",
      "Episode 260, Total Reward: -197.00, Epsilon: 0.27, Steps: 282, Rem Targets: 0.0\n",
      "Episode 270, Total Reward: -726.00, Epsilon: 0.26, Steps: 600, Rem Targets: 1.0\n",
      "Episode 280, Total Reward: -726.00, Epsilon: 0.25, Steps: 600, Rem Targets: 1.0\n",
      "Episode 290, Total Reward: -698.00, Epsilon: 0.23, Steps: 600, Rem Targets: 1.0\n",
      "Episode 300, Total Reward: -737.00, Epsilon: 0.22, Steps: 600, Rem Targets: 2.0\n",
      "Episode 310, Total Reward: -681.00, Epsilon: 0.21, Steps: 600, Rem Targets: 2.0\n",
      "Episode 320, Total Reward: -717.00, Epsilon: 0.20, Steps: 600, Rem Targets: 2.0\n",
      "Episode 330, Total Reward: -701.00, Epsilon: 0.19, Steps: 600, Rem Targets: 2.0\n",
      "Episode 340, Total Reward: -705.00, Epsilon: 0.18, Steps: 600, Rem Targets: 2.0\n",
      "Episode 350, Total Reward: -713.00, Epsilon: 0.17, Steps: 600, Rem Targets: 2.0\n",
      "Episode 360, Total Reward: -653.00, Epsilon: 0.16, Steps: 600, Rem Targets: 2.0\n",
      "Episode 370, Total Reward: -673.00, Epsilon: 0.16, Steps: 600, Rem Targets: 2.0\n",
      "Episode 380, Total Reward: -665.00, Epsilon: 0.15, Steps: 600, Rem Targets: 2.0\n",
      "Episode 390, Total Reward: -681.00, Epsilon: 0.14, Steps: 600, Rem Targets: 2.0\n",
      "Episode 400, Total Reward: -681.00, Epsilon: 0.13, Steps: 600, Rem Targets: 2.0\n",
      "Episode 410, Total Reward: -677.00, Epsilon: 0.13, Steps: 600, Rem Targets: 2.0\n",
      "Episode 420, Total Reward: -689.00, Epsilon: 0.12, Steps: 600, Rem Targets: 2.0\n",
      "Episode 430, Total Reward: -713.00, Epsilon: 0.12, Steps: 600, Rem Targets: 2.0\n",
      "Episode 440, Total Reward: -665.00, Epsilon: 0.11, Steps: 600, Rem Targets: 2.0\n",
      "Episode 450, Total Reward: -673.00, Epsilon: 0.10, Steps: 600, Rem Targets: 2.0\n",
      "Episode 460, Total Reward: -637.00, Epsilon: 0.10, Steps: 600, Rem Targets: 2.0\n",
      "Episode 470, Total Reward: -637.00, Epsilon: 0.09, Steps: 600, Rem Targets: 2.0\n",
      "Episode 480, Total Reward: -657.00, Epsilon: 0.09, Steps: 600, Rem Targets: 2.0\n",
      "Episode 490, Total Reward: -620.00, Epsilon: 0.09, Steps: 600, Rem Targets: 3.0\n",
      "Episode 500, Total Reward: -645.00, Epsilon: 0.08, Steps: 600, Rem Targets: 2.0\n",
      "Episode 510, Total Reward: -673.00, Epsilon: 0.08, Steps: 600, Rem Targets: 2.0\n",
      "Episode 520, Total Reward: -730.00, Epsilon: 0.07, Steps: 600, Rem Targets: 1.0\n",
      "Episode 530, Total Reward: -618.00, Epsilon: 0.07, Steps: 600, Rem Targets: 1.0\n",
      "Episode 540, Total Reward: -649.00, Epsilon: 0.07, Steps: 600, Rem Targets: 2.0\n",
      "Episode 550, Total Reward: -661.00, Epsilon: 0.06, Steps: 600, Rem Targets: 2.0\n",
      "Episode 560, Total Reward: -2789.00, Epsilon: 0.06, Steps: 600, Rem Targets: 2.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m state = next_state\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replay_buffer) >= batch_size:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     s_b, a_b, r_b, ns_b, d_b = \u001b[43mreplay_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     q_values = policy_net(s_b).gather(\u001b[32m1\u001b[39m, a_b.unsqueeze(\u001b[32m1\u001b[39m)).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mReplayBuffer.sample\u001b[39m\u001b[34m(self, batch_size)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# encode and stack\u001b[39;00m\n\u001b[32m     13\u001b[39m states_enc = torch.stack([encode_state_norm(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states])\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m next_states_enc = torch.stack(\u001b[43m[\u001b[49m\u001b[43mencode_state_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     16\u001b[39m     states_enc,\n\u001b[32m     17\u001b[39m     torch.tensor(actions, dtype=torch.int64, device=device),\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     torch.tensor(dones, dtype=torch.float32, device=device)\n\u001b[32m     21\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# encode and stack\u001b[39;00m\n\u001b[32m     13\u001b[39m states_enc = torch.stack([encode_state_norm(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states])\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m next_states_enc = torch.stack([\u001b[43mencode_state_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m next_states])\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     16\u001b[39m     states_enc,\n\u001b[32m     17\u001b[39m     torch.tensor(actions, dtype=torch.int64, device=device),\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     torch.tensor(dones, dtype=torch.float32, device=device)\n\u001b[32m     21\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mencode_state_norm\u001b[39m\u001b[34m(raw)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_state_norm\u001b[39m(raw: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m,\u001b[38;5;28mint\u001b[39m,\u001b[38;5;28mint\u001b[39m]) -> torch.Tensor:\n\u001b[32m      8\u001b[39m     i, j, rem = raw\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_cols\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrem\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_start\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    raw_state = env.reset()\n",
    "    state = encode_state_norm(raw_state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state.unsqueeze(0))\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "        \n",
    "        raw_next, reward, done, _ = env.step(action)\n",
    "        next_state = encode_state_norm(raw_next)\n",
    "        \n",
    "        replay_buffer.push(raw_state, action, reward, raw_next, done)\n",
    "        total_reward += reward\n",
    "        raw_state = raw_next\n",
    "        state = next_state\n",
    "        \n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            s_b, a_b, r_b, ns_b, d_b = replay_buffer.sample(batch_size)\n",
    "            q_values = policy_net(s_b).gather(1, a_b.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(ns_b).max(dim=1)[0]\n",
    "                target_q = r_b + gamma * next_q * (1 - d_b)\n",
    "            loss = nn.MSELoss()(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # decay Îµ\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    # update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    # log\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}, Steps: {step + 1}, Rem Targets: {state[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7fb585-8b80-4079-bab1-9fd2916a63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"models/A1_grid_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76bd59f8-09ed-4dc2-b174-6f5593b86d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.grid.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
