{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32da8b20-d097-45af-8af0-27ac68bb0b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from world.delivery_environment import Environment\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6b1b2b2-5a17-4270-abc2-283b8eccc4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sviatoslavgladkykh/Documents/DIC-RL-Assignment-2/world/delivery_environment.py:91: UserWarning: No reward function provided. Using default reward.\n",
      "  warn(\"No reward function provided. Using default reward.\")\n",
      "/Users/sviatoslavgladkykh/Documents/DIC-RL-Assignment-2/world/delivery_environment.py:149: UserWarning: No initial agent positions given. Randomly placing agents on the grid.\n",
      "  warn(\"No initial agent positions given. Randomly placing agents \"\n"
     ]
    }
   ],
   "source": [
    "env = Environment(Path(\"grid_configs/A1_grid.npy\"), no_gui=True)\n",
    "env.reset()\n",
    "n_rows, n_cols = env.grid.shape\n",
    "max_deliveries = env.initial_target_count\n",
    "state_dim = 3\n",
    "\n",
    "# def encode_state(state: tuple[int, int, int]) -> torch.Tensor:\n",
    "#     i, j, rem = state\n",
    "#     vec = np.zeros(state_dim, dtype=np.float32)\n",
    "#     # one-hot for row\n",
    "#     vec[i] = 1.0\n",
    "#     # one-hot for column\n",
    "#     vec[n_rows + j] = 1.0\n",
    "#     # one-hot for remaining deliveries\n",
    "#     vec[n_rows + n_cols + rem] = 1.0\n",
    "#     return torch.tensor(vec, device=device)\n",
    "\n",
    "# def encode_state(state: tuple[int, int, int]) -> torch.Tensor:\n",
    "#     i, j, rem = state\n",
    "#     return torch.tensor([i, j, rem], device=device, dtype=torch.float32)\n",
    "\n",
    "def encode_state_norm(raw: tuple[int,int,int]) -> torch.Tensor:\n",
    "    i, j, rem = raw\n",
    "    return torch.tensor([\n",
    "        i / (n_rows - 1),\n",
    "        j / (n_cols - 1),\n",
    "        rem / max_deliveries\n",
    "    ], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c832a3fb-1c71-43f0-80d6-7edf89a5400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f81470a-831b-4dc4-b252-7a1fe92d806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store raw state tuples for later encoding\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # encode and stack\n",
    "        states_enc = torch.stack([encode_state_norm(s) for s in states])\n",
    "        next_states_enc = torch.stack([encode_state_norm(s) for s in next_states])\n",
    "        return (\n",
    "            states_enc,\n",
    "            torch.tensor(actions, dtype=torch.int64, device=device),\n",
    "            torch.tensor(rewards, dtype=torch.float32, device=device),\n",
    "            next_states_enc,\n",
    "            torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "207f02f1-0591-4451-a50b-c53229a55707",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 4\n",
    "buffer_capacity = 10000\n",
    "batch_size = 128\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.997\n",
    "target_update_freq = 50\n",
    "num_episodes = 2000\n",
    "max_steps_per_episode = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f559e403-0ef2-49df-8801-c483c9e01469",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672b4947-1783-4d77-a5e4-62e37921f3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Total Reward: -2136.00, Epsilon: 0.97\n",
      "Episode 20, Total Reward: -2260.00, Epsilon: 0.94\n",
      "Episode 30, Total Reward: -1969.00, Epsilon: 0.91\n",
      "Episode 40, Total Reward: -2496.00, Epsilon: 0.89\n",
      "Episode 50, Total Reward: -2245.00, Epsilon: 0.86\n",
      "Episode 60, Total Reward: -2160.00, Epsilon: 0.84\n",
      "Episode 70, Total Reward: -1848.00, Epsilon: 0.81\n",
      "Episode 80, Total Reward: -1941.00, Epsilon: 0.79\n",
      "Episode 90, Total Reward: -2564.00, Epsilon: 0.76\n",
      "Episode 100, Total Reward: -2144.00, Epsilon: 0.74\n",
      "Episode 110, Total Reward: -2492.00, Epsilon: 0.72\n",
      "Episode 120, Total Reward: -2256.00, Epsilon: 0.70\n",
      "Episode 130, Total Reward: -2092.00, Epsilon: 0.68\n",
      "Episode 140, Total Reward: -2236.00, Epsilon: 0.66\n",
      "Episode 150, Total Reward: -2268.00, Epsilon: 0.64\n",
      "Episode 160, Total Reward: -2112.00, Epsilon: 0.62\n",
      "Episode 170, Total Reward: -2196.00, Epsilon: 0.60\n",
      "Episode 180, Total Reward: -1637.00, Epsilon: 0.58\n",
      "Episode 190, Total Reward: -1848.00, Epsilon: 0.57\n",
      "Episode 200, Total Reward: -2008.00, Epsilon: 0.55\n",
      "Episode 210, Total Reward: -2064.00, Epsilon: 0.53\n",
      "Episode 220, Total Reward: -1940.00, Epsilon: 0.52\n",
      "Episode 230, Total Reward: -1832.00, Epsilon: 0.50\n",
      "Episode 240, Total Reward: -1856.00, Epsilon: 0.49\n",
      "Episode 250, Total Reward: -1693.00, Epsilon: 0.47\n",
      "Episode 260, Total Reward: -1481.00, Epsilon: 0.46\n",
      "Episode 270, Total Reward: -1505.00, Epsilon: 0.44\n",
      "Episode 280, Total Reward: -1473.00, Epsilon: 0.43\n",
      "Episode 290, Total Reward: -1393.00, Epsilon: 0.42\n",
      "Episode 300, Total Reward: -1553.00, Epsilon: 0.41\n",
      "Episode 310, Total Reward: -1473.00, Epsilon: 0.39\n",
      "Episode 320, Total Reward: -1925.00, Epsilon: 0.38\n",
      "Episode 330, Total Reward: -1537.00, Epsilon: 0.37\n",
      "Episode 340, Total Reward: -1344.00, Epsilon: 0.36\n",
      "Episode 350, Total Reward: -1505.00, Epsilon: 0.35\n",
      "Episode 360, Total Reward: -1677.00, Epsilon: 0.34\n",
      "Episode 370, Total Reward: -1769.00, Epsilon: 0.33\n",
      "Episode 380, Total Reward: -1657.00, Epsilon: 0.32\n",
      "Episode 390, Total Reward: -1625.00, Epsilon: 0.31\n",
      "Episode 400, Total Reward: -1657.00, Epsilon: 0.30\n",
      "Episode 410, Total Reward: -1541.00, Epsilon: 0.29\n",
      "Episode 420, Total Reward: -1257.00, Epsilon: 0.28\n",
      "Episode 430, Total Reward: -1425.00, Epsilon: 0.27\n",
      "Episode 440, Total Reward: -1481.00, Epsilon: 0.27\n",
      "Episode 450, Total Reward: -1272.00, Epsilon: 0.26\n",
      "Episode 460, Total Reward: -1289.00, Epsilon: 0.25\n",
      "Episode 470, Total Reward: -1293.00, Epsilon: 0.24\n",
      "Episode 480, Total Reward: -1237.00, Epsilon: 0.24\n",
      "Episode 490, Total Reward: -1297.00, Epsilon: 0.23\n",
      "Episode 500, Total Reward: -1277.00, Epsilon: 0.22\n",
      "Episode 510, Total Reward: -1332.00, Epsilon: 0.22\n",
      "Episode 520, Total Reward: -1312.00, Epsilon: 0.21\n",
      "Episode 530, Total Reward: -1420.00, Epsilon: 0.20\n",
      "Episode 540, Total Reward: -1184.00, Epsilon: 0.20\n",
      "Episode 550, Total Reward: -1340.00, Epsilon: 0.19\n",
      "Episode 560, Total Reward: -1280.00, Epsilon: 0.19\n",
      "Episode 570, Total Reward: -1176.00, Epsilon: 0.18\n",
      "Episode 580, Total Reward: -1332.00, Epsilon: 0.18\n",
      "Episode 590, Total Reward: -1328.00, Epsilon: 0.17\n"
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_start\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    raw_state = env.reset()\n",
    "    state = encode_state_norm(raw_state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps_per_episode):\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state.unsqueeze(0))\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "        \n",
    "        raw_next, reward, done, _ = env.step(action)\n",
    "        next_state = encode_state_norm(raw_next)\n",
    "        \n",
    "        replay_buffer.push(raw_state, action, reward, raw_next, done)\n",
    "        total_reward += reward\n",
    "        raw_state = raw_next\n",
    "        state = next_state\n",
    "        \n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            s_b, a_b, r_b, ns_b, d_b = replay_buffer.sample(batch_size)\n",
    "            q_values = policy_net(s_b).gather(1, a_b.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(ns_b).max(dim=1)[0]\n",
    "                target_q = r_b + gamma * next_q * (1 - d_b)\n",
    "            loss = nn.MSELoss()(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # decay ε\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    # update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    # log\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7fb585-8b80-4079-bab1-9fd2916a63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"models/A1_grid_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76bd59f8-09ed-4dc2-b174-6f5593b86d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.grid.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
