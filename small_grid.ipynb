{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6049722-fa09-4834-859d-840a8520d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from world.delivery_environment import Environment\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4716d55f-1220-4178-98a6-79359036d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sviatoslavgladkykh/Documents/DIC-RL-Assignment-2/world/delivery_environment.py:91: UserWarning: No reward function provided. Using default reward.\n",
      "  warn(\"No reward function provided. Using default reward.\")\n",
      "/Users/sviatoslavgladkykh/Documents/DIC-RL-Assignment-2/world/delivery_environment.py:149: UserWarning: No initial agent positions given. Randomly placing agents on the grid.\n",
      "  warn(\"No initial agent positions given. Randomly placing agents \"\n"
     ]
    }
   ],
   "source": [
    "env = Environment(Path(\"grid_configs/small_grid_2.npy\"), no_gui=True)\n",
    "env.reset()\n",
    "n_rows, n_cols = env.grid.shape\n",
    "max_deliveries = env.initial_target_count\n",
    "state_dim = 3\n",
    "\n",
    "# def encode_state(state: tuple[int, int, int]) -> torch.Tensor:\n",
    "#     i, j, rem = state\n",
    "#     vec = np.zeros(state_dim, dtype=np.float32)\n",
    "#     # one-hot for row\n",
    "#     vec[i] = 1.0\n",
    "#     # one-hot for column\n",
    "#     vec[n_rows + j] = 1.0\n",
    "#     # one-hot for remaining deliveries\n",
    "#     vec[n_rows + n_cols + rem] = 1.0\n",
    "#     return torch.tensor(vec, device=device)\n",
    "\n",
    "# def encode_state(state: tuple[int, int, int]) -> torch.Tensor:\n",
    "#     i, j, rem = state\n",
    "#     return torch.tensor([i, j, rem], device=device, dtype=torch.float32)\n",
    "\n",
    "def encode_state_norm(raw: tuple[int,int,int]) -> torch.Tensor:\n",
    "    i, j, rem = raw\n",
    "    return torch.tensor([\n",
    "        i / (n_rows - 1),\n",
    "        j / (n_cols - 1),\n",
    "        rem / max_deliveries\n",
    "    ], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39650fdf-7662-46cf-a287-105e6531dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd7642b-3d85-46a2-8150-214134621441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store raw state tuples for later encoding\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # encode and stack\n",
    "        states_enc = torch.stack([encode_state_norm(s) for s in states])\n",
    "        next_states_enc = torch.stack([encode_state_norm(s) for s in next_states])\n",
    "        return (\n",
    "            states_enc,\n",
    "            torch.tensor(actions, dtype=torch.int64, device=device),\n",
    "            torch.tensor(rewards, dtype=torch.float32, device=device),\n",
    "            next_states_enc,\n",
    "            torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a479f76b-963d-492b-934f-7bcc00f51a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 4\n",
    "buffer_capacity = 5000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995\n",
    "target_update_freq = 100\n",
    "num_episodes = 500\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "policy_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3df0d5-adf2-4b00-98af-137b53722016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Total Reward: -122.00, Epsilon: 0.95\n"
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_start\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    raw_state = env.reset()                  # (row, col, remaining)\n",
    "    state = encode_state_norm(raw_state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps_per_episode):\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state.unsqueeze(0))\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "        \n",
    "        raw_next, reward, done, _ = env.step(action)\n",
    "        next_state = encode_state_norm(raw_next)\n",
    "        \n",
    "        # store transition\n",
    "        replay_buffer.push(raw_state, action, reward, raw_next, done)\n",
    "        total_reward += reward\n",
    "        raw_state = raw_next\n",
    "        state = next_state\n",
    "        \n",
    "        # learn from batch\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            s_b, a_b, r_b, ns_b, d_b = replay_buffer.sample(batch_size)\n",
    "            # Q(s,a)\n",
    "            q_values = policy_net(s_b).gather(1, a_b.unsqueeze(1)).squeeze(1)\n",
    "            # target: r + γ max_a' Q_target(s',a')\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(ns_b).max(dim=1)[0]\n",
    "                target_q = r_b + gamma * next_q * (1 - d_b)\n",
    "            loss = nn.MSELoss()(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # decay ε\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    # update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    # log\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a853ab0-87f5-4c69-b943-4b1ade00d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"small_grid_2_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b76eaf-45a0-458a-a536-172f3c35bd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a3ffb-f27f-4eed-b6f1-015782e6c579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419d493-2d6b-4868-a870-2da1a7dc104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bc91e-678b-4504-9ab2-03cde97d3234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573f65be-59db-42a5-b82b-740b94a1e887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Total Reward: -261.00, Epsilon: 0.95\n",
      "Episode 20, Total Reward: -533.00, Epsilon: 0.90\n",
      "Episode 30, Total Reward: -329.00, Epsilon: 0.86\n",
      "Episode 40, Total Reward: -1089.00, Epsilon: 0.82\n",
      "Episode 50, Total Reward: -403.00, Epsilon: 0.78\n",
      "Episode 60, Total Reward: -428.00, Epsilon: 0.74\n",
      "Episode 70, Total Reward: -494.00, Epsilon: 0.70\n",
      "Episode 80, Total Reward: -45.00, Epsilon: 0.67\n",
      "Episode 90, Total Reward: -433.00, Epsilon: 0.64\n",
      "Episode 100, Total Reward: -161.00, Epsilon: 0.61\n",
      "Episode 110, Total Reward: -440.00, Epsilon: 0.58\n",
      "Episode 120, Total Reward: -81.00, Epsilon: 0.55\n",
      "Episode 130, Total Reward: -172.00, Epsilon: 0.52\n",
      "Episode 140, Total Reward: -33.00, Epsilon: 0.50\n",
      "Episode 150, Total Reward: -10.00, Epsilon: 0.47\n",
      "Episode 160, Total Reward: -33.00, Epsilon: 0.45\n",
      "Episode 170, Total Reward: -23.00, Epsilon: 0.43\n",
      "Episode 180, Total Reward: 57.00, Epsilon: 0.41\n",
      "Episode 190, Total Reward: 63.00, Epsilon: 0.39\n",
      "Episode 200, Total Reward: 17.00, Epsilon: 0.37\n",
      "Episode 210, Total Reward: 49.00, Epsilon: 0.35\n",
      "Episode 220, Total Reward: 63.00, Epsilon: 0.33\n",
      "Episode 230, Total Reward: 49.00, Epsilon: 0.32\n",
      "Episode 240, Total Reward: -306.00, Epsilon: 0.30\n",
      "Episode 250, Total Reward: 84.00, Epsilon: 0.29\n",
      "Episode 260, Total Reward: -122.00, Epsilon: 0.27\n",
      "Episode 270, Total Reward: -708.00, Epsilon: 0.26\n",
      "Episode 280, Total Reward: -696.00, Epsilon: 0.25\n",
      "Episode 290, Total Reward: -704.00, Epsilon: 0.23\n",
      "Episode 300, Total Reward: -19.00, Epsilon: 0.22\n",
      "Episode 310, Total Reward: -656.00, Epsilon: 0.21\n",
      "Episode 320, Total Reward: 43.00, Epsilon: 0.20\n",
      "Episode 330, Total Reward: -63.00, Epsilon: 0.19\n",
      "Episode 340, Total Reward: -204.00, Epsilon: 0.18\n",
      "Episode 350, Total Reward: -5.00, Epsilon: 0.17\n",
      "Episode 360, Total Reward: -225.00, Epsilon: 0.16\n",
      "Episode 370, Total Reward: -1607.00, Epsilon: 0.16\n",
      "Episode 380, Total Reward: 82.00, Epsilon: 0.15\n",
      "Episode 390, Total Reward: -1425.00, Epsilon: 0.14\n",
      "Episode 400, Total Reward: -1905.00, Epsilon: 0.13\n",
      "Episode 410, Total Reward: 90.00, Epsilon: 0.13\n",
      "Episode 420, Total Reward: 94.00, Epsilon: 0.12\n",
      "Episode 430, Total Reward: 94.00, Epsilon: 0.12\n",
      "Episode 440, Total Reward: 89.00, Epsilon: 0.11\n",
      "Episode 450, Total Reward: 92.00, Epsilon: 0.10\n",
      "Episode 460, Total Reward: 99.00, Epsilon: 0.10\n",
      "Episode 470, Total Reward: 89.00, Epsilon: 0.10\n",
      "Episode 480, Total Reward: 88.00, Epsilon: 0.10\n",
      "Episode 490, Total Reward: 101.00, Epsilon: 0.10\n",
      "Episode 500, Total Reward: 101.00, Epsilon: 0.10\n"
     ]
    }
   ],
   "source": [
    "epsilon = epsilon_start\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    raw_state = env.reset()                  # (row, col, remaining)\n",
    "    state = encode_state_norm(raw_state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps_per_episode):\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state.unsqueeze(0))\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "        \n",
    "        raw_next, reward, done, _ = env.step(action)\n",
    "        next_state = encode_state_norm(raw_next)\n",
    "        \n",
    "        # store transition\n",
    "        replay_buffer.push(raw_state, action, reward, raw_next, done)\n",
    "        total_reward += reward\n",
    "        raw_state = raw_next\n",
    "        state = next_state\n",
    "        \n",
    "        # learn from batch\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            s_b, a_b, r_b, ns_b, d_b = replay_buffer.sample(batch_size)\n",
    "            # Q(s,a)\n",
    "            q_values = policy_net(s_b).gather(1, a_b.unsqueeze(1)).squeeze(1)\n",
    "            # target: r + γ max_a' Q_target(s',a')\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(ns_b).max(dim=1)[0]\n",
    "                target_q = r_b + gamma * next_q * (1 - d_b)\n",
    "            loss = nn.MSELoss()(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # decay ε\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    # update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    # log\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b0a469-6c20-4df9-abec-7ea369c9b0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"small_grid_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc230c8f-13a6-4177-b26e-8fe36e69b793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
