{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6049722-fa09-4834-859d-840a8520d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from world.delivery_environment import Environment\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716d55f-1220-4178-98a6-79359036d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(Path(\"grid_configs/small_grid_2.npy\"), no_gui=True)\n",
    "env.reset()\n",
    "n_rows, n_cols = env.grid.shape\n",
    "max_deliveries = env.initial_target_count\n",
    "state_dim = 3\n",
    "\n",
    "def encode_state_norm(raw: tuple[int,int,int]) -> torch.Tensor:\n",
    "    i, j, rem = raw\n",
    "    return torch.tensor([\n",
    "        i / (n_rows - 1),\n",
    "        j / (n_cols - 1),\n",
    "        rem / max_deliveries\n",
    "    ], device=device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39650fdf-7662-46cf-a287-105e6531dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim: int, n_actions: int):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7642b-3d85-46a2-8150-214134621441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # store raw state tuples for later encoding\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # encode and stack\n",
    "        states_enc = torch.stack([encode_state_norm(s) for s in states])\n",
    "        next_states_enc = torch.stack([encode_state_norm(s) for s in next_states])\n",
    "        return (\n",
    "            states_enc,\n",
    "            torch.tensor(actions, dtype=torch.int64, device=device),\n",
    "            torch.tensor(rewards, dtype=torch.float32, device=device),\n",
    "            next_states_enc,\n",
    "            torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479f76b-963d-492b-934f-7bcc00f51a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 4\n",
    "buffer_capacity = 5000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "lr = 1e-3\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995\n",
    "target_update_freq = 100\n",
    "num_episodes = 500\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "policy_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net = DQN(state_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "replay_buffer = ReplayBuffer(buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3df0d5-adf2-4b00-98af-137b53722016",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = epsilon_start\n",
    "for episode in range(1, num_episodes + 1):\n",
    "    raw_state = env.reset()                  # (row, col, remaining)\n",
    "    state = encode_state_norm(raw_state)\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps_per_episode):\n",
    "        # ε-greedy action selection\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randrange(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_vals = policy_net(state.unsqueeze(0))\n",
    "                action = q_vals.argmax(dim=1).item()\n",
    "        \n",
    "        raw_next, reward, done, _ = env.step(action)\n",
    "        next_state = encode_state_norm(raw_next)\n",
    "        \n",
    "        # store transition\n",
    "        replay_buffer.push(raw_state, action, reward, raw_next, done)\n",
    "        total_reward += reward\n",
    "        raw_state = raw_next\n",
    "        state = next_state\n",
    "        \n",
    "        # learn from batch\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            s_b, a_b, r_b, ns_b, d_b = replay_buffer.sample(batch_size)\n",
    "            # Q(s,a)\n",
    "            q_values = policy_net(s_b).gather(1, a_b.unsqueeze(1)).squeeze(1)\n",
    "            # target: r + γ max_a' Q_target(s',a')\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(ns_b).max(dim=1)[0]\n",
    "                target_q = r_b + gamma * next_q * (1 - d_b)\n",
    "            loss = nn.MSELoss()(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # decay ε\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    # update target network\n",
    "    if episode % target_update_freq == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    # log\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a853ab0-87f5-4c69-b943-4b1ade00d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"models/small_grid_2_policy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ef968-a657-4cd5-9e4b-b8c58a291570",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_net = DQN(state_dim, n_actions).to(device)\n",
    "\n",
    "eval_net.load_state_dict(torch.load(\"models/small_grid_2_policy.pt\", map_location=device))\n",
    "eval_net.eval()\n",
    "\n",
    "raw_state = env.reset()\n",
    "state = encode_state(raw_state)\n",
    "done = False\n",
    "trajectory = [env.agent_pos]\n",
    "\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        action = eval_net(state.unsqueeze(0)).argmax(dim=1).item()\n",
    "    raw_state, _, done, _ = env.step(action)\n",
    "    trajectory.append(env.agent_pos)\n",
    "    state = encode_state(raw_state)\n",
    "\n",
    "print(\"Evaluation trajectory:\", trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc230c8f-13a6-4177-b26e-8fe36e69b793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
